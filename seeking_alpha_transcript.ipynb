{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代理IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import win32com.client as win32\n",
    "import xlrd\n",
    "import json\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "import smtplib\n",
    "from proxyscrape import create_collector\n",
    "from proxyscrape import get_proxyscrape_resource\n",
    "from proxyscrape import create_collector\n",
    "import proxyscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #來源一\n",
    "# df = pd.read_excel('proxy1.xlsx',sheet_name='Sheet1')\n",
    "# ActIps_1 = []\n",
    "\n",
    "# for i in range(len(df)):\n",
    "#     ip   = str(df['IP地址'][i]).replace(' ','')\n",
    "#     port = str(df['端口'][i]).replace(' ','')\n",
    "    \n",
    "#     proxy_dict = {}\n",
    "#     key   = 'https'\n",
    "#     proxy_dict[key] ='https://'+ip+\":\"+  port\n",
    "\n",
    "#     try:\n",
    "#       # 隨機找的一篇新聞即可\n",
    "#         url = 'https://seekingalpha.com/earnings/earnings-call-transcripts'\n",
    "#         resp = requests.get(url, proxies=proxy_dict, timeout=2)\n",
    "\n",
    "#         if str(resp.status_code) == '200':\n",
    "#             ActIps_1.append(proxy_dict[key])\n",
    "#             print('Succed: {}'.format(proxy_dict[key]))\n",
    "#             resp_check = requests.get(url=\"https://blog.sodsec.com/ip.php\",proxies=proxy_dict, timeout=2)\n",
    "#             print(resp_check.text)\n",
    "#         else:\n",
    "#             print('Failed: {}'.format(proxy_dict[key]))\n",
    "\n",
    "#     except:\n",
    "#             print('Failed: {}'.format(proxy_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #來源一\n",
    "# df = pd.read_excel('proxy1.xlsx',sheet_name='Sheet2')\n",
    "\n",
    "# for i in range(len(df)):\n",
    "#     proxy   = str(df['Proxy'][i]).replace(' ','')\n",
    "\n",
    "    \n",
    "#     proxy_dict = {}\n",
    "#     key   = 'https'\n",
    "#     proxy_dict[key] ='https://'+proxy\n",
    "\n",
    "#     try:\n",
    "#       # 隨機找的一篇新聞即可\n",
    "#         url = 'https://seekingalpha.com/earnings/earnings-call-transcripts'\n",
    "#         resp = requests.get(url, proxies=proxy_dict, timeout=2)\n",
    "\n",
    "#         if str(resp.status_code) == '200':\n",
    "#             ActIps_1.append(proxy_dict[key])\n",
    "#             print('Succed: {}'.format(proxy_dict[key]))\n",
    "#             resp_check = requests.get(url=\"https://blog.sodsec.com/ip.php\",proxies=proxy_dict, timeout=2)\n",
    "#             print(resp_check.text)\n",
    "#         else:\n",
    "#             print('Failed: {}'.format(proxy_dict[key]))\n",
    "\n",
    "#     except:\n",
    "#             print('Failed: {}'.format(proxy_dict[key]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #來源一\n",
    "# df = pd.read_excel('proxy1.xlsx',sheet_name='Sheet3')\n",
    "\n",
    "# for i in range(len(df)):\n",
    "#     proxy   = str(df['Proxy'][i]).replace(' ','')\n",
    "\n",
    "    \n",
    "#     proxy_dict = {}\n",
    "#     key   = 'https'\n",
    "#     proxy_dict[key] ='socks4://'+proxy\n",
    "\n",
    "#     try:\n",
    "#       # 隨機找的一篇新聞即可\n",
    "#         url = 'https://seekingalpha.com/earnings/earnings-call-transcripts'\n",
    "#         resp = requests.get(url, proxies=proxy_dict, timeout=2)\n",
    "\n",
    "#         if str(resp.status_code) == '200':\n",
    "#             ActIps_1.append(proxy_dict[key])\n",
    "#             print('Succed: {}'.format(proxy_dict[key]))\n",
    "#             resp_check = requests.get(url=\"https://blog.sodsec.com/ip.php\",proxies=proxy_dict, timeout=2)\n",
    "#             print(resp_check.text)\n",
    "#         else:\n",
    "#             print('Failed: {}'.format(proxy_dict[key]))\n",
    "\n",
    "#     except:\n",
    "#             print('Failed: {}'.format(proxy_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#來源二\n",
    "# df = pd.read_excel('MyProxyList.xlsx')\n",
    "# df = df.dropna()\n",
    "# df.columns = [ x.replace(' ','') for x in df.columns ]\n",
    "# df = df[[ 'high-anonymous' in x for x in df['Anonymitylevel']]].reset_index(drop=True)\n",
    "\n",
    "# ActIps_2 = []\n",
    "\n",
    "# for i in range(len(df)):\n",
    "#     proxy = df['IPaddress'][i].replace(' ','')\n",
    "#     proxy_dict = {} \n",
    "#     key   = 'http'\n",
    "#     proxy_dict[key] ='http://'+proxy\n",
    "\n",
    "#     try:\n",
    "#       # 隨機找的一篇新聞即可\n",
    "#         url = 'https://seekingalpha.com/earnings/earnings-call-transcripts'\n",
    "#         resp = requests.get(url, proxies=proxy_dict, timeout=2)\n",
    "\n",
    "#         if str(resp.status_code) == '200':\n",
    "#             ActIps_2.append(proxy_dict[key])\n",
    "#             print('Succed: {}'.format(proxy_dict[key]))\n",
    "#             resp_check = requests.get(url=\"https://blog.sodsec.com/ip.php\",proxies=proxy_dict, timeout=2)\n",
    "#             print(resp_check.text)\n",
    "#         else:\n",
    "#             print('Failed: {}'.format(proxy_dict[key]))\n",
    "\n",
    "#     except:\n",
    "#             print('Failed: {}'.format(proxy_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #來源三\n",
    "# r =  requests.get('https://www.us-proxy.org/') \n",
    "# soup = BeautifulSoup(r.text, 'lxml')\n",
    "# trs = soup.select(\"#proxylisttable tr\")\n",
    "# proxy_list = []\n",
    "# for tr in trs:\n",
    "#     tds = tr.select(\"td\")\n",
    "#     if len(tds) > 6:\n",
    "#         ip = tds[0].text\n",
    "#         port = tds[1].text\n",
    "#         anonymity = tds[4].text\n",
    "#         ifScheme = tds[6].text\n",
    "#         if ifScheme == 'yes': \n",
    "#             scheme = 'https'\n",
    "#             #if anonymity == 'anonymous':\n",
    "#             proxy = \"%s://%s:%s\"%(scheme, ip, port)\n",
    "#             print(proxy)\n",
    "#             proxy_list.append(proxy)            \n",
    "#         else: \n",
    "#             scheme = 'http'\n",
    "\n",
    "# ActIps_3 = []\n",
    "# for proxy in proxy_list:\n",
    "#     proxy_dict = {}\n",
    "#     key   = proxy.split('://')[0].replace('https','http')\n",
    "#     proxy_dict[key] = proxy.replace('https','http')\n",
    "    \n",
    "#     key   = proxy.split('://')[0]\n",
    "#     proxy_dict[key] = proxy\n",
    "\n",
    "#     try:\n",
    "#       # 隨機找的一篇新聞即可\n",
    "#         url = 'https://www.chinatimes.com/realtimenews/20200205004069-260408'\n",
    "#         resp = requests.get(url, proxies=proxy_dict, timeout=2)\n",
    "\n",
    "#         if str(resp.status_code) == '200':\n",
    "#             ActIps_3.append(proxy_dict[key])\n",
    "#             print('Succed: {}'.format(proxy_dict[key]))\n",
    "#             resp_check = requests.get(url=\"https://blog.sodsec.com/ip.php\",proxies=proxy_dict, timeout=2)\n",
    "#             print(resp_check.text)\n",
    "\n",
    "#         else:\n",
    "#             print('Failed: {}'.format(proxy_dict[key]))\n",
    "#     except:\n",
    "#             print('Failed: {}'.format(proxy_dict[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ActIps_1 = []\n",
    "#來源一\n",
    "r=requests.get('https://api.proxyscrape.com/?request=displayproxies&proxytype=socks4&anonymity=elite')\n",
    "ips = [ x for x in r.text.split('\\r\\n') if  x !='']\n",
    "df = pd.DataFrame(ips,columns=['Proxy'])\n",
    "df = df.sample(frac=1, axis=0).reset_index(drop=True)\n",
    "work_count = 0\n",
    "for i in range(len(df)):\n",
    "    if work_count == 20:\n",
    "        break\n",
    "    proxy   = str(df['Proxy'][i]).replace(' ','')\n",
    "\n",
    "    \n",
    "    proxy_dict = {}\n",
    "    key   = 'https'\n",
    "    proxy_dict[key] ='socks4://'+proxy\n",
    "\n",
    "    try:\n",
    "      # 隨機找的一篇新聞即可\n",
    "        url = 'https://seekingalpha.com/earnings/earnings-call-transcripts'\n",
    "        resp = requests.get(url, proxies=proxy_dict, timeout=2)\n",
    "\n",
    "        if str(resp.status_code) == '200':\n",
    "\n",
    "\n",
    "            resp_check = requests.get(url=\"https://blog.sodsec.com/ip.php\",proxies=proxy_dict, timeout=2)\n",
    "            print(resp_check.text)\n",
    "            print('Succed: {}'.format(proxy_dict[key]))\n",
    "            ActIps_1.append(proxy_dict[key])\n",
    "            work_count += 1\n",
    "            print('work_count: {}'.format(work_count))\n",
    "        else:\n",
    "            \n",
    "            #print('Failed: {}'.format(proxy_dict[key]))\n",
    "            pass\n",
    "\n",
    "    except:\n",
    "            #print('Failed: {}'.format(proxy_dict[key]))\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "CollectorAlreadyDefinedError",
     "evalue": "socks4 is already defined as a collector",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCollectorAlreadyDefinedError\u001b[0m              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-037018abecfe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mresource_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_proxyscrape_resource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproxytype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'socks4'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manonymity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'anonymous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcollector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_collector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'socks4'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'socks4'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\proxyscrape\\proxyscrape.py\u001b[0m in \u001b[0;36mcreate_collector\u001b[1;34m(name, resource_types, refresh_interval, resources)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \"\"\"\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mCOLLECTORS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mCollectorAlreadyDefinedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{} is already defined as a collector'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_collector_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCollectorAlreadyDefinedError\u001b[0m: socks4 is already defined as a collector"
     ]
    }
   ],
   "source": [
    "\n",
    "resource_name = get_proxyscrape_resource(proxytype='socks4', anonymity='anonymous')\n",
    "collector = create_collector('socks4', 'socks4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.149.49.186\n",
      "Succed: socks4://202.136.127.14:4145\n",
      "work_count: 1\n",
      "92.51.73.14\n",
      "Succed: socks4://92.51.73.14:4153\n",
      "work_count: 2\n",
      "222.96.237.2\n",
      "Succed: socks4://211.35.73.30:4145\n",
      "work_count: 3\n",
      "27.147.225.202\n",
      "Succed: socks4://27.147.225.202:4145\n",
      "work_count: 4\n",
      "5.178.217.227\n",
      "Succed: socks4://5.178.217.227:31019\n",
      "work_count: 5\n",
      "93.64.183.162\n",
      "Succed: socks4://93.64.183.162:32889\n",
      "work_count: 6\n",
      "106.242.204.101\n",
      "Succed: socks4://106.242.204.101:4145\n",
      "work_count: 7\n",
      "177.10.144.22\n",
      "Succed: socks4://177.10.144.22:1080\n",
      "work_count: 8\n",
      "106.245.183.58\n",
      "Succed: socks4://106.245.183.58:4145\n",
      "work_count: 9\n",
      "112.218.73.138\n",
      "Succed: socks4://112.218.73.138:4145\n",
      "work_count: 10\n",
      "105.27.170.82\n",
      "Succed: socks4://105.27.170.82:4145\n",
      "work_count: 11\n",
      "200.199.114.226\n",
      "Succed: socks4://200.199.114.226:42387\n",
      "work_count: 12\n",
      "83.234.76.155\n",
      "Succed: socks4://83.234.76.155:4145\n",
      "work_count: 13\n",
      "80.241.253.210\n",
      "Succed: socks4://80.241.253.210:4145\n",
      "work_count: 14\n",
      "91.122.193.80\n",
      "Succed: socks4://91.122.193.80:4145\n",
      "work_count: 15\n",
      "82.100.63.181\n",
      "Succed: socks4://82.100.63.181:57056\n",
      "work_count: 16\n",
      "36.89.149.173\n",
      "Succed: socks4://36.67.220.55:4145\n",
      "work_count: 17\n",
      "190.109.160.73\n",
      "Succed: socks4://190.109.160.73:39734\n",
      "work_count: 18\n",
      "78.134.90.115\n",
      "Succed: socks4://78.134.90.115:51372\n",
      "work_count: 19\n"
     ]
    }
   ],
   "source": [
    "proxies = collector.get_proxies({'anonymous': True})\n",
    "ActIps_2 = []\n",
    "df = pd.DataFrame(proxies)\n",
    "work_count = 0\n",
    "for i in range(len(df)):\n",
    "    if work_count == 20:\n",
    "        break\n",
    "    proxy   = str(df['host'][i]+':'+df['port'][i]).replace(' ','')\n",
    "\n",
    "    \n",
    "    proxy_dict = {}\n",
    "    key   = 'https'\n",
    "    proxy_dict[key] ='socks4://'+proxy\n",
    "\n",
    "    try:\n",
    "      # 隨機找的一篇新聞即可\n",
    "        url = 'https://seekingalpha.com/earnings/earnings-call-transcripts'\n",
    "        resp = requests.get(url, proxies=proxy_dict, timeout=2)\n",
    "\n",
    "        if str(resp.status_code) == '200':\n",
    "\n",
    "\n",
    "            resp_check = requests.get(url=\"https://blog.sodsec.com/ip.php\",proxies=proxy_dict, timeout=2)\n",
    "            print(resp_check.text)\n",
    "            print('Succed: {}'.format(proxy_dict[key]))\n",
    "            ActIps_2.append(proxy_dict[key])\n",
    "            work_count += 1\n",
    "            print('work_count: {}'.format(work_count))\n",
    "        else:\n",
    "            \n",
    "            #print('Failed: {}'.format(proxy_dict[key]))\n",
    "            pass\n",
    "\n",
    "    except:\n",
    "            #print('Failed: {}'.format(proxy_dict[key]))\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ActIps = ActIps_1+ActIps_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ActIps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(ActIps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、進入seekingalpha 網頁 判斷con call 標題是否重複"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./transcript_title/transcript_title.txt','r',encoding='utf-8') as fp:\n",
    "     existing_articles = fp.readlines()\n",
    "fp.close()\n",
    "existing_articles  = [ x.replace('\\n','') for x in  existing_articles ]\n",
    "existing_articles_titles = [ x.split('title href : ')[0] for x in existing_articles]\n",
    "existing_articles_links =  [ x.split('title href : ')[1] for x in existing_articles]\n",
    "print(\"原標題共有 {} 項\".format(len(existing_articles_titles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get('https://seekingalpha.com/earnings/earnings-call-transcripts')\n",
    "new_articles_titles = driver.find_elements_by_class_name(\"dashboard-article-link\")\n",
    "new_articles_titles = [ x.text for x in new_articles_titles ]\n",
    "new_articles_links  = driver.find_elements_by_class_name(\"dashboard-article-link\")\n",
    "new_articles_links  = [ x.get_attribute('href') for x in new_articles_links ]\n",
    "print(\"網頁上 Con call 標題共有 {} 項\".format(len(new_articles_titles)))\n",
    "\n",
    "update_articles_titles = [ x for x in new_articles_titles  if x not in existing_articles_titles ]\n",
    "update_articles_links  = [ x for x in new_articles_links   if x not in existing_articles_links ]\n",
    "print(\"須更新標題共有 {} 項\".format(len(update_articles_titles)))\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 版本一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def change_ip_info(url,ActIps,i):\n",
    "\n",
    "\n",
    "    user_agent_list=[\n",
    "            'Mozilla/5.0(compatible;MSIE9.0;WindowsNT6.1;Trident/5.0)',\n",
    "            'Mozilla/4.0(compatible;MSIE8.0;WindowsNT6.0;Trident/4.0)',\n",
    "            'Mozilla/4.0(compatible;MSIE7.0;WindowsNT6.0)',\n",
    "            'Opera/9.80(WindowsNT6.1;U;en)Presto/2.8.131Version/11.11',\n",
    "            'Mozilla/5.0(WindowsNT6.1;rv:2.0.1)Gecko/20100101Firefox/4.0.1',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER',\n",
    "            'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)',\n",
    "            'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 SE 2.X MetaSr 1.0',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Maxthon/4.4.3.4000 Chrome/30.0.1599.101 Safari/537.36',  \n",
    "            'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.122 UBrowser/4.0.3214.0 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.221 Safari/537.36 SE 2.X MetaSr 1.0'\n",
    "        ]\n",
    "    referer_list=[\n",
    "            'https://www.sogou.com/',\n",
    "            'http://blog.csdn.net/',\n",
    "            'https://www.baidu.com/',\n",
    "             'https://www.google.com/',\n",
    "        ]\n",
    "\n",
    "    header={\n",
    "            'User-Agent':random.choice(user_agent_list), \n",
    "            'Referer':random.choice(referer_list)\n",
    "        }\n",
    "    ip=ActIps[i]\n",
    "    proxy_dict = {}\n",
    "    key   = ip.split('://')[0]\n",
    "    proxy_dict[key] = ip     \n",
    "    print(proxy_dict)\n",
    "    \n",
    "    try:\n",
    "        html=requests.get(url,headers=header, proxies=proxy_dict,timeout=(3,7))  \n",
    "        print('成功訪問')\n",
    "    except:\n",
    "        html=requests.get(url,headers=header,timeout=(3,7))  \n",
    "        print('失敗 以自己IP在訪問一次')\n",
    "    return html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sleep_min_list = [ 30 ]\n",
    "j = 0\n",
    "sleep_min = sleep_min_list[j]\n",
    "print('開始測試休眠 {} 秒'.format(300))\n",
    "\n",
    "print('開始測試 {} 秒'.format(sleep_min))\n",
    "drouble_shooting = []\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "driver_tans = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver_tans.get('https://translate.google.com.tw/')\n",
    "for i in range(len(update_articles_links)):\n",
    "    sleep =  np.random.randint(sleep_min)+np.random.random()\n",
    "    print('-------休眠 {} 秒'.format(sleep))\n",
    "    time.sleep(sleep)\n",
    "    \n",
    "    update_article_link  = update_articles_links[i]\n",
    "    update_article_title = update_articles_titles[i]\n",
    "    print(\"--------------------------------------處理連結{}----------------------------------\".format(update_article_link))\n",
    "    print(\"--------------------------------------處理連結標題{}----------------------------------\".format(update_article_title))\n",
    "    try:\n",
    "\n",
    "        r =  change_ip_info(update_article_link,ActIps,i)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        content = soup.find(id=\"page_content_wrapper\").find_all('p')\n",
    "        input_text = ''\n",
    "        mail_output_text = ''\n",
    "        mail_output_text_chi = ''\n",
    "        for x in  content:\n",
    "            #print(x.text)\n",
    "            input_text += x.text\n",
    "            if (len(input_text) > 3000)&(x != content[-1]):\n",
    "\n",
    "                driver_tans.get('https://translate.google.com.tw/')\n",
    "                time.sleep(1)\n",
    "                inputElement    = driver_tans.find_element_by_id('source')\n",
    "                inputElement.send_keys( input_text )\n",
    "                time.sleep(1.5)\n",
    "                out_put_text = driver_tans.find_element_by_xpath(\n",
    "                    \"//div[2]/div[2]/div[1]/div[2]/div[1]/div[1]/div[2]/div[3]/div[1]/div[2]/div\").text\n",
    "\n",
    "\n",
    "                #print(input_text)\n",
    "                #print(out_put_text)\n",
    "                mail_output_text_chi += out_put_text+'\\n'\n",
    "                mail_output_text += input_text+'\\n'\n",
    "                input_text = ''\n",
    "                out_put_text = ''\n",
    "            elif (len(input_text) > 3000)&(x == content[-1]):\n",
    "\n",
    "                driver_tans.get('https://translate.google.com.tw/')\n",
    "                time.sleep(1)\n",
    "                inputElement    = driver_tans.find_element_by_id('source')\n",
    "                inputElement.send_keys(input_text  )\n",
    "                time.sleep(1.5)\n",
    "                out_put_text = driver_tans.find_element_by_xpath(\n",
    "                    \"//div[2]/div[2]/div[1]/div[2]/div[1]/div[1]/div[2]/div[3]/div[1]/div[2]/div\").text\n",
    "\n",
    "\n",
    "                #print(input_text)\n",
    "                #print(out_put_text) \n",
    "                mail_output_text_chi += out_put_text+'\\n'  \n",
    "                mail_output_text += input_text+'\\n'\n",
    "                input_text = ''    \n",
    "                out_put_text = ''\n",
    "            elif (len(input_text) <= 3000)&(x == content[-1]):\n",
    "\n",
    "                driver_tans.get('https://translate.google.com.tw/')\n",
    "                time.sleep(1)\n",
    "                inputElement    = driver_tans.find_element_by_id('source')\n",
    "                inputElement.send_keys(input_text )\n",
    "                time.sleep(1.5)\n",
    "                out_put_text = driver_tans.find_element_by_xpath(\n",
    "                    \"//div[2]/div[2]/div[1]/div[2]/div[1]/div[1]/div[2]/div[3]/div[1]/div[2]/div\").text\n",
    "\n",
    "                #print(input_text)\n",
    "                #print(out_put_text) \n",
    "                mail_output_text_chi += out_put_text+'\\n'\n",
    "                mail_output_text += input_text+'\\n'\n",
    "                input_text = ''            \n",
    "                out_put_text = ''\n",
    "\n",
    "        # 開啟檔案\n",
    "        fp = open(\".//result_text//{}.txt\".format(update_article_title), \"w\",encoding='utf-8')\n",
    "        # 寫入到檔案\n",
    "        fp.write(mail_output_text)\n",
    "        # 關閉檔案\n",
    "        fp.close()\n",
    "        # 開啟檔案\n",
    "        fp = open(\".//result_text//{}.txt\".format(update_article_title+'_chi'), \"w\",encoding='utf-8')\n",
    "        # 寫入到檔案\n",
    "        fp.write(mail_output_text_chi)\n",
    "        # 關閉檔案\n",
    "        fp.close()\n",
    "        # Outlook版本------------------------------------------------------------------------------------------------------\n",
    "    #     outlook = win32.Dispatch('outlook.application')\n",
    "    #     mail = outlook.CreateItem(0)\n",
    "    #     receivers = ['Solon@cathaysite.com.tw;matt.y@cathaysite.com.tw;Xavior@cathaysite.com.tw;xavior8100483@gmail.com']\n",
    "    #     mail.To = receivers[0]\n",
    "    #     mail.Subject ='Transcript_Seekingalpha'\n",
    "    #     attachment1 = \"C://Users//User//Desktop//seekingalpha//result_text//{}.txt\".format(update_article_title)\n",
    "\n",
    "    #     mail.Attachments.Add(Source=attachment1)\n",
    "    #     mail.display()\n",
    "    #     mail.Body = out_put_text\n",
    "    #     mail.Send()\n",
    "        # Gmail版本------------------------------------------------------------------------------------------------------\n",
    "        # Account infomation load\n",
    "        gmailUser ='linshenghua82@gmail.com'\n",
    "        gmailPasswd = 'efxrbgprulmeciin'\n",
    "        to = [\"Solon@cathaysite.com.tw\",\"matt.y@cathaysite.com.tw\",\"Xavior@cathaysite.com.tw\",\"xavior8100483@gmail.com\"]\n",
    "\n",
    "        # Create message\n",
    "        message = MIMEMultipart()\n",
    "        message['Subject'] = update_article_title\n",
    "        message['From'] = gmailUser\n",
    "        #message['To'] = to\n",
    "\n",
    "        # Mail content\n",
    "        message.attach(MIMEText('{}'.format( update_article_title), 'plain', 'utf-8'))\n",
    "\n",
    "        # File\n",
    "        file = MIMEText(open(\"C://Users//User//Desktop//seekingalpha//result_text//{}.txt\".format(update_article_title), 'r', encoding='utf-8').read(), 'base64', 'utf-8')\n",
    "        file['Content-Type'] = 'application/octet-stream'\n",
    "        file['Content-Disposition'] = 'attachment; filename='+\"{}.txt\".format(update_article_title)\n",
    "        message.attach(file)\n",
    "        file = MIMEText(open(\"C://Users//User//Desktop//seekingalpha//result_text//{}.txt\".format(update_article_title+'_chi'), 'r', encoding='utf-8').read(), 'base64', 'utf-8')\n",
    "        file['Content-Type'] = 'application/octet-stream'\n",
    "        file['Content-Disposition'] = 'attachment; filename='+\"{}.txt\".format(update_article_title+'_chi')\n",
    "        message.attach(file)\n",
    "        # Set smtp\n",
    "        smtp = smtplib.SMTP(\"smtp.gmail.com:587\")\n",
    "        smtp.ehlo()\n",
    "        smtp.starttls()\n",
    "        smtp.login(gmailUser, gmailPasswd)\n",
    "\n",
    "        # Send mail\n",
    "        smtp.sendmail(message['From'], to, message.as_string())\n",
    "        print('Send mails OK!')\n",
    "        \n",
    "        \n",
    "        # 將所有新標題寫入到檔案\n",
    "        with open('./transcript_title/transcript_title.txt','a',encoding='utf-8') as fp:\n",
    "            lines = update_article_title+'title href : '+update_article_link+'\\n' \n",
    "            fp.writelines(lines)\n",
    "        fp.close()\n",
    "        driver.close()\n",
    "    except:\n",
    "        drouble_shooting.append(update_article_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
